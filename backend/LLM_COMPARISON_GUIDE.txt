================================================================================
                    LLM COMPARISON TEST GUIDE
                    Ollama vs vLLM Performance
================================================================================

PURPOSE
================================================================================
Test your system's ability to answer 100 hard-level questions about:
- Data quality & anomalies
- Login patterns & frequency
- Cross-document matching
- Security anomaly detection
- Complex statistical queries

Compare performance between:
- Ollama (LLaMA 3.2) - Your current setup
- vLLM (LLaMA 3.2) - High-performance inference server


SETUP INSTRUCTIONS
================================================================================

OPTION 1: Test with Ollama Only (Current Setup)
--------------------------------------------------------------------------------
1. Make sure Ollama is running:
   curl http://localhost:11434

2. Run test:
   cd backend
   node test-llm-comparison.js

3. Results will show Ollama performance only


OPTION 2: Test Ollama vs vLLM (Full Comparison)
--------------------------------------------------------------------------------

Step 1: Install vLLM
   pip install vllm

Step 2: Start vLLM server (new terminal):
   python -m vllm.entrypoints.openai.api_server \
     --model meta-llama/Llama-3.2-3B-Instruct \
     --port 8000

Step 3: Verify vLLM is running:
   curl http://localhost:8000/v1/models

Step 4: Run comparison test:
   cd backend
   node test-llm-comparison.js

Step 5: Check results:
   - Console output shows real-time comparison
   - llm-comparison-results.json has detailed results


WHAT THE TEST DOES
================================================================================

For each of 20 hard questions:
1. Searches your data using vectorService
2. Builds context from relevant documents
3. Sends question + context to Ollama
4. Sends same question + context to vLLM
5. Measures response time for both
6. Compares answer quality

Metrics Tracked:
- Response time (ms)
- Answer accuracy
- Total processing time
- Average speed per question


EXPECTED RESULTS
================================================================================

Ollama (LLaMA 3.2):
- Response time: 2000-5000ms per question
- Good accuracy for simple queries
- Slower for complex analysis
- Memory: ~4GB

vLLM (LLaMA 3.2):
- Response time: 500-2000ms per question
- Same accuracy as Ollama
- 2-5x faster inference
- Better batching
- Memory: ~6GB


SAMPLE QUESTIONS TESTED
================================================================================

Easy:
- "Which AGENTID has the highest number of login records?"
- "What is the earliest login timestamp?"

Medium:
- "How many agents are associated with cultureholidays.com?"
- "Which agent logged in twice within 10 seconds?"

Hard:
- "Which agents logged in more than 5 times within 10 minutes?"
- "Which email has a typo .comm instead of .com?"
- "Which agent shows credential sharing behavior?"


INTERPRETING RESULTS
================================================================================

Speed Comparison:
- If vLLM is 2-3x faster: Normal
- If vLLM is 5x+ faster: Excellent optimization
- If Ollama is faster: vLLM not properly configured

Accuracy Comparison:
- Both should give similar answers
- vLLM may be slightly more consistent
- Check llm-comparison-results.json for details

Quality Metrics:
- Correct answer: 10 points
- Partially correct: 5 points
- Wrong answer: 0 points
- No answer: -5 points


TROUBLESHOOTING
================================================================================

Error: "Cannot connect to Ollama"
Solution: Start Ollama - ollama serve

Error: "Cannot connect to vLLM"
Solution: 
1. Install vLLM: pip install vllm
2. Start server: python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-3.2-3B-Instruct --port 8000

Error: "Out of memory"
Solution: 
- Reduce batch size
- Use smaller model
- Close other applications

Error: "Model not found"
Solution:
- Ollama: ollama pull llama3.2
- vLLM: Downloads automatically on first run


ADVANCED: Custom Questions
================================================================================

Edit test-questions.js to add your own questions:

const testQuestions = [
  "Your custom question 1",
  "Your custom question 2",
  // ... add more
];


PERFORMANCE OPTIMIZATION
================================================================================

For Ollama:
- Use GPU if available
- Increase context window
- Adjust temperature (0.7 default)

For vLLM:
- Enable tensor parallelism: --tensor-parallel-size 2
- Use quantization: --quantization awq
- Batch requests: --max-num-batched-tokens 8192


REAL-WORLD USE CASES
================================================================================

Use Ollama when:
- Development/testing
- Low query volume (<10/min)
- Simple setup needed
- Limited resources

Use vLLM when:
- Production deployment
- High query volume (100+/min)
- Need low latency (<1s)
- Have GPU resources
- Multiple concurrent users


COST COMPARISON
================================================================================

Ollama:
- Free, open source
- Runs locally
- No API costs
- Hardware: $0-$2000 (GPU optional)

vLLM:
- Free, open source
- Runs locally or cloud
- No API costs
- Hardware: $500-$5000 (GPU recommended)
- Cloud: $0.50-$2.00 per hour (GPU instance)


NEXT STEPS
================================================================================

After testing:

1. If vLLM is significantly faster:
   - Consider using vLLM for production
   - Keep Ollama for development

2. If answers are inaccurate:
   - Improve prompts in queryController.js
   - Add more context to retrieval
   - Fine-tune model on your data

3. If both are slow:
   - Optimize vectorService search
   - Reduce context size
   - Use smaller model
   - Add caching

4. For production:
   - Set up load balancer
   - Add request queue
   - Implement caching
   - Monitor performance


RUNNING ALL 100 QUESTIONS
================================================================================

To test all 100 hard questions:

1. Create full question list in test-questions.js
2. Increase timeout in test script
3. Run: node test-llm-comparison.js
4. Expected time: 
   - Ollama: 3-8 minutes
   - vLLM: 1-3 minutes

5. Results saved to llm-comparison-results.json


CONCLUSION
================================================================================

This test helps you:
✓ Compare LLM performance
✓ Identify bottlenecks
✓ Choose best LLM for production
✓ Optimize prompts and context
✓ Validate answer accuracy

For 100 hard questions, expect:
- Ollama: 60-80% accuracy, 5-10 min total
- vLLM: 60-80% accuracy, 2-4 min total

Both use same model, so accuracy should be similar.
Main difference is speed and scalability.

================================================================================
